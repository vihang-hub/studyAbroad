"""
AI service for report generation using Gemini 2.0 Flash
Uses LangChain for orchestration with UK-specific prompts
"""

import json
import time
from datetime import datetime
from typing import AsyncIterator
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from src.config import settings
from src.api.models.report import ReportContent, ReportSection, Citation
from src.api.services.sla_monitor import get_sla_monitor
from logging_lib.logger import get_logger

logger = get_logger()

# Initialize Gemini model
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash-exp",
    google_api_key=settings.GEMINI_API_KEY,
    temperature=0.3,  # Low temperature for factual accuracy
)

# UK-specific system prompt
UK_SYSTEM_PROMPT = """You are an expert educational consultant specializing in UK higher education and migration.

CRITICAL REQUIREMENTS:
1. ALL information must be specific to the United Kingdom ONLY
2. Include citations for every major claim (minimum 3 citations per section, except Executive Summary)
3. Generate exactly 10 sections in this EXACT order:

   SECTION 1: Executive Summary (5-10 bullet points)
   SECTION 2: Study Options in the UK
   SECTION 3: Estimated Cost of Studying (Tuition Ranges + Living Costs)
   SECTION 4: Visa & Immigration Overview (high-level, non-legal)
   SECTION 5: Post-Study Work Options
   SECTION 6: Job Prospects in the Chosen Subject
   SECTION 7: Fallback Job Prospects (Out-of-Field)
   SECTION 8: Risks & Reality Check
   SECTION 9: 30/60/90-Day Action Plan
   SECTION 10: Sources & Citations

4. Each section (except Executive Summary) must be 200-300 words
5. Executive Summary must be 5-10 concise bullet points
6. Citations must include: title, url, and snippet
7. All URLs must be real and verifiable
8. Focus on current 2024-2025 academic year information
9. If data is uncertain, state uncertainty clearly - NO uncited confident claims allowed

FORMAT YOUR RESPONSE AS VALID JSON:
{
  "query": "user's original query",
  "summary": "brief 2-3 sentence summary",
  "sections": [
    {
      "heading": "Executive Summary",
      "content": "• Bullet point 1\\n• Bullet point 2\\n• Bullet point 3\\n...",
      "citations": []
    },
    {
      "heading": "Study Options in the UK",
      "content": "detailed content with markdown formatting (200-300 words)",
      "citations": [
        {
          "title": "source title",
          "url": "https://...",
          "snippet": "relevant quote or summary"
        }
      ]
    }
  ]
}
"""
from inspect import signature as _mutmut_signature
from typing import Annotated
from typing import Callable
from typing import ClassVar


MutantDict = Annotated[dict[str, Callable], "Mutant"]


def _mutmut_trampoline(orig, mutants, call_args, call_kwargs, self_arg = None):
    """Forward call to original or mutated function, depending on the environment"""
    import os
    mutant_under_test = os.environ['MUTANT_UNDER_TEST']
    if mutant_under_test == 'fail':
        from mutmut.__main__ import MutmutProgrammaticFailException
        raise MutmutProgrammaticFailException('Failed programmatically')      
    elif mutant_under_test == 'stats':
        from mutmut.__main__ import record_trampoline_hit
        record_trampoline_hit(orig.__module__ + '.' + orig.__name__)
        result = orig(*call_args, **call_kwargs)
        return result
    prefix = orig.__module__ + '.' + orig.__name__ + '__mutmut_'
    if not mutant_under_test.startswith(prefix):
        result = orig(*call_args, **call_kwargs)
        return result
    mutant_name = mutant_under_test.rpartition('.')[-1]
    if self_arg is not None:
        # call to a class method where self is not bound
        result = mutants[mutant_name](self_arg, *call_args, **call_kwargs)
    else:
        result = mutants[mutant_name](*call_args, **call_kwargs)
    return result


async def x_generate_report__mutmut_orig(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_1(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_2(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(None):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_3(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            None
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_4(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "XXQuery must be related to studying in the United Kingdom. XX"
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_5(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "query must be related to studying in the united kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_6(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "QUERY MUST BE RELATED TO STUDYING IN THE UNITED KINGDOM. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_7(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "XXPlease specify UK universities, courses, or migration.XX"
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_8(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "please specify uk universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_9(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "PLEASE SPECIFY UK UNIVERSITIES, COURSES, OR MIGRATION."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_10(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = None

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_11(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = None

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_12(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=None),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_13(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=None),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_14(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = None
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_15(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(None)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_16(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = None

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_17(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = None

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_18(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(None)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_19(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = None
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_20(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get(None, []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_21(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", None):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_22(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get([]):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_23(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", ):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_24(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("XXsectionsXX", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_25(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("SECTIONS", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_26(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = None

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_27(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=None,
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_28(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=None,
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_29(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=None,
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_30(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=None,
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_31(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_32(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_33(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_34(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_35(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["XXtitleXX"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_36(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["TITLE"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_37(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["XXurlXX"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_38(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["URL"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_39(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get(None, ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_40(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", None),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_41(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get(""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_42(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_43(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("XXsnippetXX", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_44(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("SNIPPET", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_45(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", "XXXX"),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_46(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get(None, [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_47(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", None)
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_48(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get([])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_49(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", )
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_50(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("XXcitationsXX", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_51(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("CITATIONS", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_52(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                None
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_53(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=None,
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_54(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=None,
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_55(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=None,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_56(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_57(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_58(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_59(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["XXheadingXX"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_60(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["HEADING"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_61(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["XXcontentXX"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_62(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["CONTENT"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_63(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) == 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_64(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 11:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_65(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                None
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_66(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = None
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_67(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(None)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_68(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations != 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_69(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 1:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_70(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError(None)

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_71(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("XXReport must include citations for credibilityXX")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_72(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_73(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("REPORT MUST INCLUDE CITATIONS FOR CREDIBILITY")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_74(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=None,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_75(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=None,
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_76(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=None,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_77(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=None,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_78(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=None,
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_79(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_80(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_81(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_82(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_83(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_84(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get(None, ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_85(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", None),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_86(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get(""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_87(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_88(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("XXsummaryXX", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_89(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("SUMMARY", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_90(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", "XXXX"),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_91(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError(None)
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_92(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("XXFailed to parse AI response as JSONXX")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_93(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("failed to parse ai response as json")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_94(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("FAILED TO PARSE AI RESPONSE AS JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(e)}")


async def x_generate_report__mutmut_95(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(None)


async def x_generate_report__mutmut_96(query: str) -> ReportContent:
    """
    Generate a complete research report for UK study query
    Returns structured ReportContent with citations
    """
    # Validate UK-only query
    if not is_uk_query(query):
        raise ValueError(
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )

    # Create prompt
    prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

    try:
        # Generate response
        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        response = llm.invoke(messages)
        content = response.content

        # Parse JSON response
        report_data = json.loads(content)

        # Validate and structure the response
        sections = []
        for section_data in report_data.get("sections", []):
            citations = [
                Citation(
                    title=cit["title"],
                    url=cit["url"],
                    snippet=cit.get("snippet", ""),
                    accessed_at=datetime.utcnow(),
                )
                for cit in section_data.get("citations", [])
            ]

            sections.append(
                ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
            )

        # Validate we have all 10 sections
        if len(sections) != 10:
            raise ValueError(
                f"Expected 10 sections, got {len(sections)}. Report generation incomplete."
            )

        # Validate citations exist
        total_citations = sum(len(s.citations) for s in sections)
        if total_citations == 0:
            raise ValueError("Report must include citations for credibility")

        return ReportContent(
            query=query,
            summary=report_data.get("summary", ""),
            sections=sections,
            total_citations=total_citations,
            generated_at=datetime.utcnow(),
        )

    except json.JSONDecodeError:
        raise ValueError("Failed to parse AI response as JSON")
    except Exception as e:
        raise Exception(f"Report generation failed: {str(None)}")

x_generate_report__mutmut_mutants : ClassVar[MutantDict] = {
'x_generate_report__mutmut_1': x_generate_report__mutmut_1, 
    'x_generate_report__mutmut_2': x_generate_report__mutmut_2, 
    'x_generate_report__mutmut_3': x_generate_report__mutmut_3, 
    'x_generate_report__mutmut_4': x_generate_report__mutmut_4, 
    'x_generate_report__mutmut_5': x_generate_report__mutmut_5, 
    'x_generate_report__mutmut_6': x_generate_report__mutmut_6, 
    'x_generate_report__mutmut_7': x_generate_report__mutmut_7, 
    'x_generate_report__mutmut_8': x_generate_report__mutmut_8, 
    'x_generate_report__mutmut_9': x_generate_report__mutmut_9, 
    'x_generate_report__mutmut_10': x_generate_report__mutmut_10, 
    'x_generate_report__mutmut_11': x_generate_report__mutmut_11, 
    'x_generate_report__mutmut_12': x_generate_report__mutmut_12, 
    'x_generate_report__mutmut_13': x_generate_report__mutmut_13, 
    'x_generate_report__mutmut_14': x_generate_report__mutmut_14, 
    'x_generate_report__mutmut_15': x_generate_report__mutmut_15, 
    'x_generate_report__mutmut_16': x_generate_report__mutmut_16, 
    'x_generate_report__mutmut_17': x_generate_report__mutmut_17, 
    'x_generate_report__mutmut_18': x_generate_report__mutmut_18, 
    'x_generate_report__mutmut_19': x_generate_report__mutmut_19, 
    'x_generate_report__mutmut_20': x_generate_report__mutmut_20, 
    'x_generate_report__mutmut_21': x_generate_report__mutmut_21, 
    'x_generate_report__mutmut_22': x_generate_report__mutmut_22, 
    'x_generate_report__mutmut_23': x_generate_report__mutmut_23, 
    'x_generate_report__mutmut_24': x_generate_report__mutmut_24, 
    'x_generate_report__mutmut_25': x_generate_report__mutmut_25, 
    'x_generate_report__mutmut_26': x_generate_report__mutmut_26, 
    'x_generate_report__mutmut_27': x_generate_report__mutmut_27, 
    'x_generate_report__mutmut_28': x_generate_report__mutmut_28, 
    'x_generate_report__mutmut_29': x_generate_report__mutmut_29, 
    'x_generate_report__mutmut_30': x_generate_report__mutmut_30, 
    'x_generate_report__mutmut_31': x_generate_report__mutmut_31, 
    'x_generate_report__mutmut_32': x_generate_report__mutmut_32, 
    'x_generate_report__mutmut_33': x_generate_report__mutmut_33, 
    'x_generate_report__mutmut_34': x_generate_report__mutmut_34, 
    'x_generate_report__mutmut_35': x_generate_report__mutmut_35, 
    'x_generate_report__mutmut_36': x_generate_report__mutmut_36, 
    'x_generate_report__mutmut_37': x_generate_report__mutmut_37, 
    'x_generate_report__mutmut_38': x_generate_report__mutmut_38, 
    'x_generate_report__mutmut_39': x_generate_report__mutmut_39, 
    'x_generate_report__mutmut_40': x_generate_report__mutmut_40, 
    'x_generate_report__mutmut_41': x_generate_report__mutmut_41, 
    'x_generate_report__mutmut_42': x_generate_report__mutmut_42, 
    'x_generate_report__mutmut_43': x_generate_report__mutmut_43, 
    'x_generate_report__mutmut_44': x_generate_report__mutmut_44, 
    'x_generate_report__mutmut_45': x_generate_report__mutmut_45, 
    'x_generate_report__mutmut_46': x_generate_report__mutmut_46, 
    'x_generate_report__mutmut_47': x_generate_report__mutmut_47, 
    'x_generate_report__mutmut_48': x_generate_report__mutmut_48, 
    'x_generate_report__mutmut_49': x_generate_report__mutmut_49, 
    'x_generate_report__mutmut_50': x_generate_report__mutmut_50, 
    'x_generate_report__mutmut_51': x_generate_report__mutmut_51, 
    'x_generate_report__mutmut_52': x_generate_report__mutmut_52, 
    'x_generate_report__mutmut_53': x_generate_report__mutmut_53, 
    'x_generate_report__mutmut_54': x_generate_report__mutmut_54, 
    'x_generate_report__mutmut_55': x_generate_report__mutmut_55, 
    'x_generate_report__mutmut_56': x_generate_report__mutmut_56, 
    'x_generate_report__mutmut_57': x_generate_report__mutmut_57, 
    'x_generate_report__mutmut_58': x_generate_report__mutmut_58, 
    'x_generate_report__mutmut_59': x_generate_report__mutmut_59, 
    'x_generate_report__mutmut_60': x_generate_report__mutmut_60, 
    'x_generate_report__mutmut_61': x_generate_report__mutmut_61, 
    'x_generate_report__mutmut_62': x_generate_report__mutmut_62, 
    'x_generate_report__mutmut_63': x_generate_report__mutmut_63, 
    'x_generate_report__mutmut_64': x_generate_report__mutmut_64, 
    'x_generate_report__mutmut_65': x_generate_report__mutmut_65, 
    'x_generate_report__mutmut_66': x_generate_report__mutmut_66, 
    'x_generate_report__mutmut_67': x_generate_report__mutmut_67, 
    'x_generate_report__mutmut_68': x_generate_report__mutmut_68, 
    'x_generate_report__mutmut_69': x_generate_report__mutmut_69, 
    'x_generate_report__mutmut_70': x_generate_report__mutmut_70, 
    'x_generate_report__mutmut_71': x_generate_report__mutmut_71, 
    'x_generate_report__mutmut_72': x_generate_report__mutmut_72, 
    'x_generate_report__mutmut_73': x_generate_report__mutmut_73, 
    'x_generate_report__mutmut_74': x_generate_report__mutmut_74, 
    'x_generate_report__mutmut_75': x_generate_report__mutmut_75, 
    'x_generate_report__mutmut_76': x_generate_report__mutmut_76, 
    'x_generate_report__mutmut_77': x_generate_report__mutmut_77, 
    'x_generate_report__mutmut_78': x_generate_report__mutmut_78, 
    'x_generate_report__mutmut_79': x_generate_report__mutmut_79, 
    'x_generate_report__mutmut_80': x_generate_report__mutmut_80, 
    'x_generate_report__mutmut_81': x_generate_report__mutmut_81, 
    'x_generate_report__mutmut_82': x_generate_report__mutmut_82, 
    'x_generate_report__mutmut_83': x_generate_report__mutmut_83, 
    'x_generate_report__mutmut_84': x_generate_report__mutmut_84, 
    'x_generate_report__mutmut_85': x_generate_report__mutmut_85, 
    'x_generate_report__mutmut_86': x_generate_report__mutmut_86, 
    'x_generate_report__mutmut_87': x_generate_report__mutmut_87, 
    'x_generate_report__mutmut_88': x_generate_report__mutmut_88, 
    'x_generate_report__mutmut_89': x_generate_report__mutmut_89, 
    'x_generate_report__mutmut_90': x_generate_report__mutmut_90, 
    'x_generate_report__mutmut_91': x_generate_report__mutmut_91, 
    'x_generate_report__mutmut_92': x_generate_report__mutmut_92, 
    'x_generate_report__mutmut_93': x_generate_report__mutmut_93, 
    'x_generate_report__mutmut_94': x_generate_report__mutmut_94, 
    'x_generate_report__mutmut_95': x_generate_report__mutmut_95, 
    'x_generate_report__mutmut_96': x_generate_report__mutmut_96
}

def generate_report(*args, **kwargs):
    result = _mutmut_trampoline(x_generate_report__mutmut_orig, x_generate_report__mutmut_mutants, args, kwargs)
    return result 

generate_report.__signature__ = _mutmut_signature(x_generate_report__mutmut_orig)
x_generate_report__mutmut_orig.__name__ = 'x_generate_report'


async def x_generate_report_stream__mutmut_orig(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_1(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = None
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_2(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = ""
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_3(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = None

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_4(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        None,
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_5(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        None
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_6(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_7(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_8(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "XXReport generation startedXX",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_9(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_10(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "REPORT GENERATION STARTED",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_11(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "XXreport_idXX": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_12(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "REPORT_ID": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_13(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "XXqueryXX": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_14(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "QUERY": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_15(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "XXstart_timeXX": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_16(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "START_TIME": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_17(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_18(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(None):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_19(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = None
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_20(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "XXQuery must be related to studying in the United Kingdom. XX"
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_21(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "query must be related to studying in the united kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_22(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "QUERY MUST BE RELATED TO STUDYING IN THE UNITED KINGDOM. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_23(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "XXPlease specify UK universities, courses, or migration.XX"
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_24(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "please specify uk universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_25(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "PLEASE SPECIFY UK UNIVERSITIES, COURSES, OR MIGRATION."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_26(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps(None)
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_27(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"XXtypeXX": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_28(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"TYPE": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_29(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "XXerrorXX", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_30(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "ERROR", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_31(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "XXmessageXX": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_32(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "MESSAGE": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_33(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = None

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_34(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model=None,
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_35(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=None,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_36(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=None,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_37(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=None,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_38(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_39(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_40(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_41(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_42(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="XXgemini-2.0-flash-expXX",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_43(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="GEMINI-2.0-FLASH-EXP",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_44(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=1.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_45(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=False,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_46(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = None

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_47(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = None

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_48(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=None),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_49(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=None),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_50(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = None

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_51(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = "XXXX"

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_52(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(None):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_53(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = None
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_54(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response = content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_55(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response -= content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_56(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None or content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_57(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is not None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_58(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = None
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_59(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = None
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_60(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) / 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_61(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time + start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_62(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1001
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_63(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    None,
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_64(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    None
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_65(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_66(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_67(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "XXFirst token receivedXX",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_68(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "first token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_69(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "FIRST TOKEN RECEIVED",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_70(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "XXreport_idXX": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_71(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "REPORT_ID": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_72(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "XXfirst_token_latency_msXX": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_73(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "FIRST_TOKEN_LATENCY_MS": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_74(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(None, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_75(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, None),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_76(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_77(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, ),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_78(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 3),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_79(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                None
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_80(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "XXtypeXX": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_81(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "TYPE": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_82(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "XXchunkXX",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_83(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "CHUNK",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_84(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "XXcontentXX": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_85(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "CONTENT": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_86(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "XXreport_idXX": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_87(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "REPORT_ID": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_88(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = None

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_89(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(None)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_90(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = None
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_91(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(None, 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_92(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), None):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_93(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_94(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), ):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_95(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get(None, []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_96(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", None), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_97(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get([]), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_98(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", ), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_99(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("XXsectionsXX", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_100(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("SECTIONS", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_101(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 2):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_102(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = None

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_103(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=None,
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_104(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=None,
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_105(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=None,
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_106(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=None,
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_107(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_108(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_109(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_110(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_111(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["XXtitleXX"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_112(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["TITLE"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_113(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["XXurlXX"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_114(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["URL"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_115(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get(None, ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_116(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", None),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_117(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get(""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_118(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_119(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("XXsnippetXX", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_120(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("SNIPPET", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_121(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", "XXXX"),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_122(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get(None, [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_123(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", None)
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_124(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get([])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_125(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", )
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_126(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("XXcitationsXX", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_127(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("CITATIONS", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_128(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = None
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_129(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=None,
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_130(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=None,
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_131(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=None,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_132(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_133(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_134(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_135(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["XXheadingXX"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_136(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["HEADING"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_137(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["XXcontentXX"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_138(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["CONTENT"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_139(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(None)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_140(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    None
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_141(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "XXtypeXX": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_142(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "TYPE": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_143(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "XXsectionXX",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_144(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "SECTION",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_145(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "XXsection_numXX": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_146(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "SECTION_NUM": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_147(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "XXheadingXX": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_148(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "HEADING": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_149(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "XXcontentXX": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_150(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "CONTENT": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_151(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "XXcitationsXX": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_152(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "CITATIONS": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_153(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = None
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_154(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(None)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_155(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=None,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_156(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=None,
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_157(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=None,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_158(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=None,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_159(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=None,
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_160(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_161(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_162(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_163(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_164(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_165(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get(None, ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_166(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", None),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_167(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get(""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_168(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_169(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("XXsummaryXX", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_170(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("SUMMARY", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_171(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", "XXXX"),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_172(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = None
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_173(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=None,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_174(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=None,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_175(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=None,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_176(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=None
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_177(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_178(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_179(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_180(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_181(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                None,
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_182(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                None
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_183(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_184(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_185(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "XXReport generation completed successfullyXX",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_186(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_187(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "REPORT GENERATION COMPLETED SUCCESSFULLY",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_188(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "XXreport_idXX": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_189(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "REPORT_ID": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_190(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "XXcompletion_timeXX": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_191(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "COMPLETION_TIME": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_192(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "XXtotal_latency_msXX": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_193(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "TOTAL_LATENCY_MS": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_194(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round(None, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_195(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, None),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_196(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round(2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_197(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, ),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_198(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) / 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_199(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time + start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_200(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1001, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_201(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 3),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_202(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = None
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_203(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=None,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_204(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=None,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_205(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=None,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_206(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=None
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_207(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_208(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_209(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_210(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_211(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                None
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_212(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"XXtypeXX": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_213(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"TYPE": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_214(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "XXerrorXX", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_215(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "ERROR", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_216(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "XXmessageXX": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_217(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "MESSAGE": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_218(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(None)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_219(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = None
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_220(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=None,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_221(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=None,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_222(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=None,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_223(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=None
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_224(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_225(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_226(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_227(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_228(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(None)
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_229(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"XXtypeXX": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_230(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"TYPE": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_231(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "XXerrorXX", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_232(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "ERROR", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_233(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "XXmessageXX": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_234(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "MESSAGE": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_235(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(None)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_236(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = None
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_237(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=None,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_238(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=None,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_239(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=None,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_240(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=None
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_241(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_242(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_243(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_244(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_245(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps(None)
        return


async def x_generate_report_stream__mutmut_246(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"XXtypeXX": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_247(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"TYPE": "error", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_248(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "XXerrorXX", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_249(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "ERROR", "message": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_250(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "XXmessageXX": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_251(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "MESSAGE": f"Report generation failed: {str(e)}"})
        return


async def x_generate_report_stream__mutmut_252(report_id: str, query: str) -> AsyncIterator[str]:
    """
    Generate report with streaming support per specification Section 5 & 9
    Uses LangChain streaming to yield sections progressively (Gemini-style)

    Yields JSON chunks in SSE format:
    - {"type": "section", "section_num": N, "heading": "...", "content": "...", "citations": [...]}
    - {"type": "complete", "report_id": "..."}
    - {"type": "error", "message": "..."}
    """
    # T172a-b: Track streaming SLA metrics
    start_time = time.time()
    first_token_time: float | None = None
    sla_monitor = get_sla_monitor()

    logger.info(
        "Report generation started",
        {
            "report_id": report_id,
            "query": query,
            "start_time": start_time,
        }
    )

    # Validate UK-only query
    if not is_uk_query(query):
        error_msg = (
            "Query must be related to studying in the United Kingdom. "
            "Please specify UK universities, courses, or migration."
        )
        yield json.dumps({"type": "error", "message": error_msg})
        return

    try:
        # Create streaming-enabled LLM
        llm_stream = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=settings.GEMINI_API_KEY,
            temperature=0.3,
            streaming=True,  # Enable streaming
        )

        # Create prompt
        prompt = f"""Generate a comprehensive research report for the following UK study query:

QUERY: {query}

Provide detailed, factual information with proper citations.
Remember: UK-specific information only!"""

        messages = [
            SystemMessage(content=UK_SYSTEM_PROMPT),
            HumanMessage(content=prompt),
        ]

        # Accumulate response chunks
        full_response = ""

        # Stream response from LLM
        async for chunk in llm_stream.astream(messages):
            content = chunk.content
            full_response += content

            # T172b: Record first token time
            if first_token_time is None and content:
                first_token_time = time.time()
                first_token_latency_ms = (first_token_time - start_time) * 1000
                logger.info(
                    "First token received",
                    {
                        "report_id": report_id,
                        "first_token_latency_ms": round(first_token_latency_ms, 2),
                    }
                )

            # Yield raw chunk for progressive rendering
            # Note: This yields incremental text, not complete sections
            # Frontend should accumulate and parse complete JSON
            yield json.dumps(
                {
                    "type": "chunk",
                    "content": content,
                    "report_id": report_id,
                }
            )

        # Parse complete response
        try:
            report_data = json.loads(full_response)

            # Validate and yield complete sections
            sections = []
            for i, section_data in enumerate(report_data.get("sections", []), 1):
                citations = [
                    Citation(
                        title=cit["title"],
                        url=cit["url"],
                        snippet=cit.get("snippet", ""),
                        accessed_at=datetime.utcnow(),
                    )
                    for cit in section_data.get("citations", [])
                ]

                section = ReportSection(
                    heading=section_data["heading"],
                    content=section_data["content"],
                    citations=citations,
                )
                sections.append(section)

                # Yield complete section
                yield json.dumps(
                    {
                        "type": "section",
                        "section_num": i,
                        "heading": section.heading,
                        "content": section.content,
                        "citations": [c.dict() for c in section.citations],
                    }
                )

            # Validate report structure (will raise if invalid)
            total_citations = sum(len(s.citations) for s in sections)
            # Validation via model instantiation
            ReportContent(
                query=query,
                summary=report_data.get("summary", ""),
                sections=sections,
                total_citations=total_citations,
                generated_at=datetime.utcnow(),
            )

            # Store complete report (TODO: integrate with report_service)
            # await store_report(report_id, ReportContent(...))

            # T172a: Record successful completion
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )

            logger.info(
                "Report generation completed successfully",
                {
                    "report_id": report_id,
                    "completion_time": completion_time,
                    "total_latency_ms": round((completion_time - start_time) * 1000, 2),
                }
            )

        except json.JSONDecodeError as e:
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps(
                {"type": "error", "message": f"Failed to parse AI response as JSON: {str(e)}"}
            )
            return
        except ValueError as e:
            # Pydantic validation error (wrong sections, missing citations, etc.)
            completion_time = time.time()
            sla_monitor.record_streaming_latency(
                report_id=report_id,
                start_time=start_time,
                first_token_time=first_token_time,
                completion_time=completion_time
            )
            yield json.dumps({"type": "error", "message": f"Report validation failed: {str(e)}"})
            return

    except Exception as e:
        completion_time = time.time()
        sla_monitor.record_streaming_latency(
            report_id=report_id,
            start_time=start_time,
            first_token_time=first_token_time,
            completion_time=completion_time
        )
        yield json.dumps({"type": "error", "message": f"Report generation failed: {str(None)}"})
        return

x_generate_report_stream__mutmut_mutants : ClassVar[MutantDict] = {
'x_generate_report_stream__mutmut_1': x_generate_report_stream__mutmut_1, 
    'x_generate_report_stream__mutmut_2': x_generate_report_stream__mutmut_2, 
    'x_generate_report_stream__mutmut_3': x_generate_report_stream__mutmut_3, 
    'x_generate_report_stream__mutmut_4': x_generate_report_stream__mutmut_4, 
    'x_generate_report_stream__mutmut_5': x_generate_report_stream__mutmut_5, 
    'x_generate_report_stream__mutmut_6': x_generate_report_stream__mutmut_6, 
    'x_generate_report_stream__mutmut_7': x_generate_report_stream__mutmut_7, 
    'x_generate_report_stream__mutmut_8': x_generate_report_stream__mutmut_8, 
    'x_generate_report_stream__mutmut_9': x_generate_report_stream__mutmut_9, 
    'x_generate_report_stream__mutmut_10': x_generate_report_stream__mutmut_10, 
    'x_generate_report_stream__mutmut_11': x_generate_report_stream__mutmut_11, 
    'x_generate_report_stream__mutmut_12': x_generate_report_stream__mutmut_12, 
    'x_generate_report_stream__mutmut_13': x_generate_report_stream__mutmut_13, 
    'x_generate_report_stream__mutmut_14': x_generate_report_stream__mutmut_14, 
    'x_generate_report_stream__mutmut_15': x_generate_report_stream__mutmut_15, 
    'x_generate_report_stream__mutmut_16': x_generate_report_stream__mutmut_16, 
    'x_generate_report_stream__mutmut_17': x_generate_report_stream__mutmut_17, 
    'x_generate_report_stream__mutmut_18': x_generate_report_stream__mutmut_18, 
    'x_generate_report_stream__mutmut_19': x_generate_report_stream__mutmut_19, 
    'x_generate_report_stream__mutmut_20': x_generate_report_stream__mutmut_20, 
    'x_generate_report_stream__mutmut_21': x_generate_report_stream__mutmut_21, 
    'x_generate_report_stream__mutmut_22': x_generate_report_stream__mutmut_22, 
    'x_generate_report_stream__mutmut_23': x_generate_report_stream__mutmut_23, 
    'x_generate_report_stream__mutmut_24': x_generate_report_stream__mutmut_24, 
    'x_generate_report_stream__mutmut_25': x_generate_report_stream__mutmut_25, 
    'x_generate_report_stream__mutmut_26': x_generate_report_stream__mutmut_26, 
    'x_generate_report_stream__mutmut_27': x_generate_report_stream__mutmut_27, 
    'x_generate_report_stream__mutmut_28': x_generate_report_stream__mutmut_28, 
    'x_generate_report_stream__mutmut_29': x_generate_report_stream__mutmut_29, 
    'x_generate_report_stream__mutmut_30': x_generate_report_stream__mutmut_30, 
    'x_generate_report_stream__mutmut_31': x_generate_report_stream__mutmut_31, 
    'x_generate_report_stream__mutmut_32': x_generate_report_stream__mutmut_32, 
    'x_generate_report_stream__mutmut_33': x_generate_report_stream__mutmut_33, 
    'x_generate_report_stream__mutmut_34': x_generate_report_stream__mutmut_34, 
    'x_generate_report_stream__mutmut_35': x_generate_report_stream__mutmut_35, 
    'x_generate_report_stream__mutmut_36': x_generate_report_stream__mutmut_36, 
    'x_generate_report_stream__mutmut_37': x_generate_report_stream__mutmut_37, 
    'x_generate_report_stream__mutmut_38': x_generate_report_stream__mutmut_38, 
    'x_generate_report_stream__mutmut_39': x_generate_report_stream__mutmut_39, 
    'x_generate_report_stream__mutmut_40': x_generate_report_stream__mutmut_40, 
    'x_generate_report_stream__mutmut_41': x_generate_report_stream__mutmut_41, 
    'x_generate_report_stream__mutmut_42': x_generate_report_stream__mutmut_42, 
    'x_generate_report_stream__mutmut_43': x_generate_report_stream__mutmut_43, 
    'x_generate_report_stream__mutmut_44': x_generate_report_stream__mutmut_44, 
    'x_generate_report_stream__mutmut_45': x_generate_report_stream__mutmut_45, 
    'x_generate_report_stream__mutmut_46': x_generate_report_stream__mutmut_46, 
    'x_generate_report_stream__mutmut_47': x_generate_report_stream__mutmut_47, 
    'x_generate_report_stream__mutmut_48': x_generate_report_stream__mutmut_48, 
    'x_generate_report_stream__mutmut_49': x_generate_report_stream__mutmut_49, 
    'x_generate_report_stream__mutmut_50': x_generate_report_stream__mutmut_50, 
    'x_generate_report_stream__mutmut_51': x_generate_report_stream__mutmut_51, 
    'x_generate_report_stream__mutmut_52': x_generate_report_stream__mutmut_52, 
    'x_generate_report_stream__mutmut_53': x_generate_report_stream__mutmut_53, 
    'x_generate_report_stream__mutmut_54': x_generate_report_stream__mutmut_54, 
    'x_generate_report_stream__mutmut_55': x_generate_report_stream__mutmut_55, 
    'x_generate_report_stream__mutmut_56': x_generate_report_stream__mutmut_56, 
    'x_generate_report_stream__mutmut_57': x_generate_report_stream__mutmut_57, 
    'x_generate_report_stream__mutmut_58': x_generate_report_stream__mutmut_58, 
    'x_generate_report_stream__mutmut_59': x_generate_report_stream__mutmut_59, 
    'x_generate_report_stream__mutmut_60': x_generate_report_stream__mutmut_60, 
    'x_generate_report_stream__mutmut_61': x_generate_report_stream__mutmut_61, 
    'x_generate_report_stream__mutmut_62': x_generate_report_stream__mutmut_62, 
    'x_generate_report_stream__mutmut_63': x_generate_report_stream__mutmut_63, 
    'x_generate_report_stream__mutmut_64': x_generate_report_stream__mutmut_64, 
    'x_generate_report_stream__mutmut_65': x_generate_report_stream__mutmut_65, 
    'x_generate_report_stream__mutmut_66': x_generate_report_stream__mutmut_66, 
    'x_generate_report_stream__mutmut_67': x_generate_report_stream__mutmut_67, 
    'x_generate_report_stream__mutmut_68': x_generate_report_stream__mutmut_68, 
    'x_generate_report_stream__mutmut_69': x_generate_report_stream__mutmut_69, 
    'x_generate_report_stream__mutmut_70': x_generate_report_stream__mutmut_70, 
    'x_generate_report_stream__mutmut_71': x_generate_report_stream__mutmut_71, 
    'x_generate_report_stream__mutmut_72': x_generate_report_stream__mutmut_72, 
    'x_generate_report_stream__mutmut_73': x_generate_report_stream__mutmut_73, 
    'x_generate_report_stream__mutmut_74': x_generate_report_stream__mutmut_74, 
    'x_generate_report_stream__mutmut_75': x_generate_report_stream__mutmut_75, 
    'x_generate_report_stream__mutmut_76': x_generate_report_stream__mutmut_76, 
    'x_generate_report_stream__mutmut_77': x_generate_report_stream__mutmut_77, 
    'x_generate_report_stream__mutmut_78': x_generate_report_stream__mutmut_78, 
    'x_generate_report_stream__mutmut_79': x_generate_report_stream__mutmut_79, 
    'x_generate_report_stream__mutmut_80': x_generate_report_stream__mutmut_80, 
    'x_generate_report_stream__mutmut_81': x_generate_report_stream__mutmut_81, 
    'x_generate_report_stream__mutmut_82': x_generate_report_stream__mutmut_82, 
    'x_generate_report_stream__mutmut_83': x_generate_report_stream__mutmut_83, 
    'x_generate_report_stream__mutmut_84': x_generate_report_stream__mutmut_84, 
    'x_generate_report_stream__mutmut_85': x_generate_report_stream__mutmut_85, 
    'x_generate_report_stream__mutmut_86': x_generate_report_stream__mutmut_86, 
    'x_generate_report_stream__mutmut_87': x_generate_report_stream__mutmut_87, 
    'x_generate_report_stream__mutmut_88': x_generate_report_stream__mutmut_88, 
    'x_generate_report_stream__mutmut_89': x_generate_report_stream__mutmut_89, 
    'x_generate_report_stream__mutmut_90': x_generate_report_stream__mutmut_90, 
    'x_generate_report_stream__mutmut_91': x_generate_report_stream__mutmut_91, 
    'x_generate_report_stream__mutmut_92': x_generate_report_stream__mutmut_92, 
    'x_generate_report_stream__mutmut_93': x_generate_report_stream__mutmut_93, 
    'x_generate_report_stream__mutmut_94': x_generate_report_stream__mutmut_94, 
    'x_generate_report_stream__mutmut_95': x_generate_report_stream__mutmut_95, 
    'x_generate_report_stream__mutmut_96': x_generate_report_stream__mutmut_96, 
    'x_generate_report_stream__mutmut_97': x_generate_report_stream__mutmut_97, 
    'x_generate_report_stream__mutmut_98': x_generate_report_stream__mutmut_98, 
    'x_generate_report_stream__mutmut_99': x_generate_report_stream__mutmut_99, 
    'x_generate_report_stream__mutmut_100': x_generate_report_stream__mutmut_100, 
    'x_generate_report_stream__mutmut_101': x_generate_report_stream__mutmut_101, 
    'x_generate_report_stream__mutmut_102': x_generate_report_stream__mutmut_102, 
    'x_generate_report_stream__mutmut_103': x_generate_report_stream__mutmut_103, 
    'x_generate_report_stream__mutmut_104': x_generate_report_stream__mutmut_104, 
    'x_generate_report_stream__mutmut_105': x_generate_report_stream__mutmut_105, 
    'x_generate_report_stream__mutmut_106': x_generate_report_stream__mutmut_106, 
    'x_generate_report_stream__mutmut_107': x_generate_report_stream__mutmut_107, 
    'x_generate_report_stream__mutmut_108': x_generate_report_stream__mutmut_108, 
    'x_generate_report_stream__mutmut_109': x_generate_report_stream__mutmut_109, 
    'x_generate_report_stream__mutmut_110': x_generate_report_stream__mutmut_110, 
    'x_generate_report_stream__mutmut_111': x_generate_report_stream__mutmut_111, 
    'x_generate_report_stream__mutmut_112': x_generate_report_stream__mutmut_112, 
    'x_generate_report_stream__mutmut_113': x_generate_report_stream__mutmut_113, 
    'x_generate_report_stream__mutmut_114': x_generate_report_stream__mutmut_114, 
    'x_generate_report_stream__mutmut_115': x_generate_report_stream__mutmut_115, 
    'x_generate_report_stream__mutmut_116': x_generate_report_stream__mutmut_116, 
    'x_generate_report_stream__mutmut_117': x_generate_report_stream__mutmut_117, 
    'x_generate_report_stream__mutmut_118': x_generate_report_stream__mutmut_118, 
    'x_generate_report_stream__mutmut_119': x_generate_report_stream__mutmut_119, 
    'x_generate_report_stream__mutmut_120': x_generate_report_stream__mutmut_120, 
    'x_generate_report_stream__mutmut_121': x_generate_report_stream__mutmut_121, 
    'x_generate_report_stream__mutmut_122': x_generate_report_stream__mutmut_122, 
    'x_generate_report_stream__mutmut_123': x_generate_report_stream__mutmut_123, 
    'x_generate_report_stream__mutmut_124': x_generate_report_stream__mutmut_124, 
    'x_generate_report_stream__mutmut_125': x_generate_report_stream__mutmut_125, 
    'x_generate_report_stream__mutmut_126': x_generate_report_stream__mutmut_126, 
    'x_generate_report_stream__mutmut_127': x_generate_report_stream__mutmut_127, 
    'x_generate_report_stream__mutmut_128': x_generate_report_stream__mutmut_128, 
    'x_generate_report_stream__mutmut_129': x_generate_report_stream__mutmut_129, 
    'x_generate_report_stream__mutmut_130': x_generate_report_stream__mutmut_130, 
    'x_generate_report_stream__mutmut_131': x_generate_report_stream__mutmut_131, 
    'x_generate_report_stream__mutmut_132': x_generate_report_stream__mutmut_132, 
    'x_generate_report_stream__mutmut_133': x_generate_report_stream__mutmut_133, 
    'x_generate_report_stream__mutmut_134': x_generate_report_stream__mutmut_134, 
    'x_generate_report_stream__mutmut_135': x_generate_report_stream__mutmut_135, 
    'x_generate_report_stream__mutmut_136': x_generate_report_stream__mutmut_136, 
    'x_generate_report_stream__mutmut_137': x_generate_report_stream__mutmut_137, 
    'x_generate_report_stream__mutmut_138': x_generate_report_stream__mutmut_138, 
    'x_generate_report_stream__mutmut_139': x_generate_report_stream__mutmut_139, 
    'x_generate_report_stream__mutmut_140': x_generate_report_stream__mutmut_140, 
    'x_generate_report_stream__mutmut_141': x_generate_report_stream__mutmut_141, 
    'x_generate_report_stream__mutmut_142': x_generate_report_stream__mutmut_142, 
    'x_generate_report_stream__mutmut_143': x_generate_report_stream__mutmut_143, 
    'x_generate_report_stream__mutmut_144': x_generate_report_stream__mutmut_144, 
    'x_generate_report_stream__mutmut_145': x_generate_report_stream__mutmut_145, 
    'x_generate_report_stream__mutmut_146': x_generate_report_stream__mutmut_146, 
    'x_generate_report_stream__mutmut_147': x_generate_report_stream__mutmut_147, 
    'x_generate_report_stream__mutmut_148': x_generate_report_stream__mutmut_148, 
    'x_generate_report_stream__mutmut_149': x_generate_report_stream__mutmut_149, 
    'x_generate_report_stream__mutmut_150': x_generate_report_stream__mutmut_150, 
    'x_generate_report_stream__mutmut_151': x_generate_report_stream__mutmut_151, 
    'x_generate_report_stream__mutmut_152': x_generate_report_stream__mutmut_152, 
    'x_generate_report_stream__mutmut_153': x_generate_report_stream__mutmut_153, 
    'x_generate_report_stream__mutmut_154': x_generate_report_stream__mutmut_154, 
    'x_generate_report_stream__mutmut_155': x_generate_report_stream__mutmut_155, 
    'x_generate_report_stream__mutmut_156': x_generate_report_stream__mutmut_156, 
    'x_generate_report_stream__mutmut_157': x_generate_report_stream__mutmut_157, 
    'x_generate_report_stream__mutmut_158': x_generate_report_stream__mutmut_158, 
    'x_generate_report_stream__mutmut_159': x_generate_report_stream__mutmut_159, 
    'x_generate_report_stream__mutmut_160': x_generate_report_stream__mutmut_160, 
    'x_generate_report_stream__mutmut_161': x_generate_report_stream__mutmut_161, 
    'x_generate_report_stream__mutmut_162': x_generate_report_stream__mutmut_162, 
    'x_generate_report_stream__mutmut_163': x_generate_report_stream__mutmut_163, 
    'x_generate_report_stream__mutmut_164': x_generate_report_stream__mutmut_164, 
    'x_generate_report_stream__mutmut_165': x_generate_report_stream__mutmut_165, 
    'x_generate_report_stream__mutmut_166': x_generate_report_stream__mutmut_166, 
    'x_generate_report_stream__mutmut_167': x_generate_report_stream__mutmut_167, 
    'x_generate_report_stream__mutmut_168': x_generate_report_stream__mutmut_168, 
    'x_generate_report_stream__mutmut_169': x_generate_report_stream__mutmut_169, 
    'x_generate_report_stream__mutmut_170': x_generate_report_stream__mutmut_170, 
    'x_generate_report_stream__mutmut_171': x_generate_report_stream__mutmut_171, 
    'x_generate_report_stream__mutmut_172': x_generate_report_stream__mutmut_172, 
    'x_generate_report_stream__mutmut_173': x_generate_report_stream__mutmut_173, 
    'x_generate_report_stream__mutmut_174': x_generate_report_stream__mutmut_174, 
    'x_generate_report_stream__mutmut_175': x_generate_report_stream__mutmut_175, 
    'x_generate_report_stream__mutmut_176': x_generate_report_stream__mutmut_176, 
    'x_generate_report_stream__mutmut_177': x_generate_report_stream__mutmut_177, 
    'x_generate_report_stream__mutmut_178': x_generate_report_stream__mutmut_178, 
    'x_generate_report_stream__mutmut_179': x_generate_report_stream__mutmut_179, 
    'x_generate_report_stream__mutmut_180': x_generate_report_stream__mutmut_180, 
    'x_generate_report_stream__mutmut_181': x_generate_report_stream__mutmut_181, 
    'x_generate_report_stream__mutmut_182': x_generate_report_stream__mutmut_182, 
    'x_generate_report_stream__mutmut_183': x_generate_report_stream__mutmut_183, 
    'x_generate_report_stream__mutmut_184': x_generate_report_stream__mutmut_184, 
    'x_generate_report_stream__mutmut_185': x_generate_report_stream__mutmut_185, 
    'x_generate_report_stream__mutmut_186': x_generate_report_stream__mutmut_186, 
    'x_generate_report_stream__mutmut_187': x_generate_report_stream__mutmut_187, 
    'x_generate_report_stream__mutmut_188': x_generate_report_stream__mutmut_188, 
    'x_generate_report_stream__mutmut_189': x_generate_report_stream__mutmut_189, 
    'x_generate_report_stream__mutmut_190': x_generate_report_stream__mutmut_190, 
    'x_generate_report_stream__mutmut_191': x_generate_report_stream__mutmut_191, 
    'x_generate_report_stream__mutmut_192': x_generate_report_stream__mutmut_192, 
    'x_generate_report_stream__mutmut_193': x_generate_report_stream__mutmut_193, 
    'x_generate_report_stream__mutmut_194': x_generate_report_stream__mutmut_194, 
    'x_generate_report_stream__mutmut_195': x_generate_report_stream__mutmut_195, 
    'x_generate_report_stream__mutmut_196': x_generate_report_stream__mutmut_196, 
    'x_generate_report_stream__mutmut_197': x_generate_report_stream__mutmut_197, 
    'x_generate_report_stream__mutmut_198': x_generate_report_stream__mutmut_198, 
    'x_generate_report_stream__mutmut_199': x_generate_report_stream__mutmut_199, 
    'x_generate_report_stream__mutmut_200': x_generate_report_stream__mutmut_200, 
    'x_generate_report_stream__mutmut_201': x_generate_report_stream__mutmut_201, 
    'x_generate_report_stream__mutmut_202': x_generate_report_stream__mutmut_202, 
    'x_generate_report_stream__mutmut_203': x_generate_report_stream__mutmut_203, 
    'x_generate_report_stream__mutmut_204': x_generate_report_stream__mutmut_204, 
    'x_generate_report_stream__mutmut_205': x_generate_report_stream__mutmut_205, 
    'x_generate_report_stream__mutmut_206': x_generate_report_stream__mutmut_206, 
    'x_generate_report_stream__mutmut_207': x_generate_report_stream__mutmut_207, 
    'x_generate_report_stream__mutmut_208': x_generate_report_stream__mutmut_208, 
    'x_generate_report_stream__mutmut_209': x_generate_report_stream__mutmut_209, 
    'x_generate_report_stream__mutmut_210': x_generate_report_stream__mutmut_210, 
    'x_generate_report_stream__mutmut_211': x_generate_report_stream__mutmut_211, 
    'x_generate_report_stream__mutmut_212': x_generate_report_stream__mutmut_212, 
    'x_generate_report_stream__mutmut_213': x_generate_report_stream__mutmut_213, 
    'x_generate_report_stream__mutmut_214': x_generate_report_stream__mutmut_214, 
    'x_generate_report_stream__mutmut_215': x_generate_report_stream__mutmut_215, 
    'x_generate_report_stream__mutmut_216': x_generate_report_stream__mutmut_216, 
    'x_generate_report_stream__mutmut_217': x_generate_report_stream__mutmut_217, 
    'x_generate_report_stream__mutmut_218': x_generate_report_stream__mutmut_218, 
    'x_generate_report_stream__mutmut_219': x_generate_report_stream__mutmut_219, 
    'x_generate_report_stream__mutmut_220': x_generate_report_stream__mutmut_220, 
    'x_generate_report_stream__mutmut_221': x_generate_report_stream__mutmut_221, 
    'x_generate_report_stream__mutmut_222': x_generate_report_stream__mutmut_222, 
    'x_generate_report_stream__mutmut_223': x_generate_report_stream__mutmut_223, 
    'x_generate_report_stream__mutmut_224': x_generate_report_stream__mutmut_224, 
    'x_generate_report_stream__mutmut_225': x_generate_report_stream__mutmut_225, 
    'x_generate_report_stream__mutmut_226': x_generate_report_stream__mutmut_226, 
    'x_generate_report_stream__mutmut_227': x_generate_report_stream__mutmut_227, 
    'x_generate_report_stream__mutmut_228': x_generate_report_stream__mutmut_228, 
    'x_generate_report_stream__mutmut_229': x_generate_report_stream__mutmut_229, 
    'x_generate_report_stream__mutmut_230': x_generate_report_stream__mutmut_230, 
    'x_generate_report_stream__mutmut_231': x_generate_report_stream__mutmut_231, 
    'x_generate_report_stream__mutmut_232': x_generate_report_stream__mutmut_232, 
    'x_generate_report_stream__mutmut_233': x_generate_report_stream__mutmut_233, 
    'x_generate_report_stream__mutmut_234': x_generate_report_stream__mutmut_234, 
    'x_generate_report_stream__mutmut_235': x_generate_report_stream__mutmut_235, 
    'x_generate_report_stream__mutmut_236': x_generate_report_stream__mutmut_236, 
    'x_generate_report_stream__mutmut_237': x_generate_report_stream__mutmut_237, 
    'x_generate_report_stream__mutmut_238': x_generate_report_stream__mutmut_238, 
    'x_generate_report_stream__mutmut_239': x_generate_report_stream__mutmut_239, 
    'x_generate_report_stream__mutmut_240': x_generate_report_stream__mutmut_240, 
    'x_generate_report_stream__mutmut_241': x_generate_report_stream__mutmut_241, 
    'x_generate_report_stream__mutmut_242': x_generate_report_stream__mutmut_242, 
    'x_generate_report_stream__mutmut_243': x_generate_report_stream__mutmut_243, 
    'x_generate_report_stream__mutmut_244': x_generate_report_stream__mutmut_244, 
    'x_generate_report_stream__mutmut_245': x_generate_report_stream__mutmut_245, 
    'x_generate_report_stream__mutmut_246': x_generate_report_stream__mutmut_246, 
    'x_generate_report_stream__mutmut_247': x_generate_report_stream__mutmut_247, 
    'x_generate_report_stream__mutmut_248': x_generate_report_stream__mutmut_248, 
    'x_generate_report_stream__mutmut_249': x_generate_report_stream__mutmut_249, 
    'x_generate_report_stream__mutmut_250': x_generate_report_stream__mutmut_250, 
    'x_generate_report_stream__mutmut_251': x_generate_report_stream__mutmut_251, 
    'x_generate_report_stream__mutmut_252': x_generate_report_stream__mutmut_252
}

def generate_report_stream(*args, **kwargs):
    result = _mutmut_trampoline(x_generate_report_stream__mutmut_orig, x_generate_report_stream__mutmut_mutants, args, kwargs)
    return result 

generate_report_stream.__signature__ = _mutmut_signature(x_generate_report_stream__mutmut_orig)
x_generate_report_stream__mutmut_orig.__name__ = 'x_generate_report_stream'


def x_is_uk_query__mutmut_orig(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_1(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = None

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_2(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "XXukXX",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_3(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "UK",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_4(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "XXunited kingdomXX",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_5(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "UNITED KINGDOM",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_6(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "XXbritainXX",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_7(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "BRITAIN",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_8(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "XXbritishXX",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_9(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "BRITISH",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_10(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "XXenglandXX",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_11(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "ENGLAND",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_12(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "XXscotlandXX",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_13(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "SCOTLAND",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_14(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "XXwalesXX",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_15(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "WALES",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_16(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "XXnorthern irelandXX",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_17(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "NORTHERN IRELAND",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_18(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "XXlondonXX",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_19(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "LONDON",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_20(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "XXoxfordXX",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_21(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "OXFORD",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_22(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "XXcambridgeXX",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_23(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "CAMBRIDGE",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_24(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "XXrussell groupXX",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_25(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "RUSSELL GROUP",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_26(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "XXucasXX",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_27(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "UCAS",
    ]

    query_lower = query.lower()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_28(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = None
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_29(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.upper()
    return any(keyword in query_lower for keyword in uk_keywords)


def x_is_uk_query__mutmut_30(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(None)


def x_is_uk_query__mutmut_31(query: str) -> bool:
    """
    Validate that query is related to UK study/migration
    """
    uk_keywords = [
        "uk",
        "united kingdom",
        "britain",
        "british",
        "england",
        "scotland",
        "wales",
        "northern ireland",
        "london",
        "oxford",
        "cambridge",
        "russell group",
        "ucas",
    ]

    query_lower = query.lower()
    return any(keyword not in query_lower for keyword in uk_keywords)

x_is_uk_query__mutmut_mutants : ClassVar[MutantDict] = {
'x_is_uk_query__mutmut_1': x_is_uk_query__mutmut_1, 
    'x_is_uk_query__mutmut_2': x_is_uk_query__mutmut_2, 
    'x_is_uk_query__mutmut_3': x_is_uk_query__mutmut_3, 
    'x_is_uk_query__mutmut_4': x_is_uk_query__mutmut_4, 
    'x_is_uk_query__mutmut_5': x_is_uk_query__mutmut_5, 
    'x_is_uk_query__mutmut_6': x_is_uk_query__mutmut_6, 
    'x_is_uk_query__mutmut_7': x_is_uk_query__mutmut_7, 
    'x_is_uk_query__mutmut_8': x_is_uk_query__mutmut_8, 
    'x_is_uk_query__mutmut_9': x_is_uk_query__mutmut_9, 
    'x_is_uk_query__mutmut_10': x_is_uk_query__mutmut_10, 
    'x_is_uk_query__mutmut_11': x_is_uk_query__mutmut_11, 
    'x_is_uk_query__mutmut_12': x_is_uk_query__mutmut_12, 
    'x_is_uk_query__mutmut_13': x_is_uk_query__mutmut_13, 
    'x_is_uk_query__mutmut_14': x_is_uk_query__mutmut_14, 
    'x_is_uk_query__mutmut_15': x_is_uk_query__mutmut_15, 
    'x_is_uk_query__mutmut_16': x_is_uk_query__mutmut_16, 
    'x_is_uk_query__mutmut_17': x_is_uk_query__mutmut_17, 
    'x_is_uk_query__mutmut_18': x_is_uk_query__mutmut_18, 
    'x_is_uk_query__mutmut_19': x_is_uk_query__mutmut_19, 
    'x_is_uk_query__mutmut_20': x_is_uk_query__mutmut_20, 
    'x_is_uk_query__mutmut_21': x_is_uk_query__mutmut_21, 
    'x_is_uk_query__mutmut_22': x_is_uk_query__mutmut_22, 
    'x_is_uk_query__mutmut_23': x_is_uk_query__mutmut_23, 
    'x_is_uk_query__mutmut_24': x_is_uk_query__mutmut_24, 
    'x_is_uk_query__mutmut_25': x_is_uk_query__mutmut_25, 
    'x_is_uk_query__mutmut_26': x_is_uk_query__mutmut_26, 
    'x_is_uk_query__mutmut_27': x_is_uk_query__mutmut_27, 
    'x_is_uk_query__mutmut_28': x_is_uk_query__mutmut_28, 
    'x_is_uk_query__mutmut_29': x_is_uk_query__mutmut_29, 
    'x_is_uk_query__mutmut_30': x_is_uk_query__mutmut_30, 
    'x_is_uk_query__mutmut_31': x_is_uk_query__mutmut_31
}

def is_uk_query(*args, **kwargs):
    result = _mutmut_trampoline(x_is_uk_query__mutmut_orig, x_is_uk_query__mutmut_mutants, args, kwargs)
    return result 

is_uk_query.__signature__ = _mutmut_signature(x_is_uk_query__mutmut_orig)
x_is_uk_query__mutmut_orig.__name__ = 'x_is_uk_query'
